{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_data = pd.read_csv('training.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data.columns =['Class', 'Data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>that's not even an exaggeration ) and at midni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                               Data\n",
       "0      1  this was the first clive cussler i've ever rea...\n",
       "1      1                   i liked the Da Vinci Code a lot.\n",
       "2      1                   i liked the Da Vinci Code a lot.\n",
       "3      1  I liked the Da Vinci Code but it ultimatly did...\n",
       "4      1  that's not even an exaggeration ) and at midni..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeld_data = pd.read_csv('testdata.csv', sep=',')\n",
    "unlabeld_data.columns = ['Data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>harvard is dumb, i mean they really have to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm loving Shanghai &gt; &gt; &gt; ^ _ ^.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harvard is for dumb people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As i stepped out of my beautiful Toyota, i hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bodies being dismembered, blown apart, and mut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data\n",
       "0  harvard is dumb, i mean they really have to be...\n",
       "1                   I'm loving Shanghai > > > ^ _ ^.\n",
       "2                        harvard is for dumb people.\n",
       "3  As i stepped out of my beautiful Toyota, i hea...\n",
       "4  Bodies being dismembered, blown apart, and mut..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeld_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "sentiment_data = shuffle(sentiment_data)\n",
    "unlabeld_data = shuffle(unlabeld_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>1</td>\n",
       "      <td>Mission Impossible 3 was excellent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1</td>\n",
       "      <td>I love The Da Vinci Code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6217</th>\n",
       "      <td>0</td>\n",
       "      <td>Brokeback Mountain is fucking horrible..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>1</td>\n",
       "      <td>I love Brokeback Mountain.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>1</td>\n",
       "      <td>dudeee i LOVED brokeback mountain!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                      Data\n",
       "1481      1       Mission Impossible 3 was excellent.\n",
       "705       1               I love The Da Vinci Code...\n",
       "6217      0  Brokeback Mountain is fucking horrible..\n",
       "3572      1                I love Brokeback Mountain.\n",
       "3578      1     dudeee i LOVED brokeback mountain!!!!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sentiment_data.iloc[:, 0].values\n",
    "reviews = sentiment_data.iloc[:, 1].values\n",
    "unlabeled_reviews = unlabeld_data.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I absolutely love my MacBook Pro.'\n",
      " \"And as stupid as San Francisco's road system is, we weren't able to turn back because of how all the roads are one-way streets.\"\n",
      " 'TAKE THAT STUPID UCLA!!!!!!!..' ...\n",
      " 'It was really ironic that he spent the first part of class talking about his own professor at Harvard who was a pompous arrogant ass.'\n",
      " 'San Francisco was brilliant, Sausalito...'\n",
      " \"We talked about Purdue's awful music / culture scene, the Flaming Lips ripping off Cat Stevens, Lollapalooza ( he was there too ), stuff like that.\"]\n"
     ]
    }
   ],
   "source": [
    "print(unlabeled_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_processed = []\n",
    "unlabeled_processed = [] \n",
    "for review in reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    reviews_processed.append(review_cool_one)\n",
    "    \n",
    "for review in unlabeled_reviews:\n",
    "    review_cool_one = ''.join([char for char in review if char not in punctuation])\n",
    "    unlabeled_processed.append(review_cool_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_reviews = []\n",
    "word_unlabeled = []\n",
    "all_words = []\n",
    "for review in reviews_processed:\n",
    "    word_reviews.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "\n",
    "for review in unlabeled_processed:\n",
    "    word_unlabeled.append(review.lower().split())\n",
    "    for word in review.split():\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "counter = Counter(all_words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word: i for i, word in enumerate(vocab, 1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_to_ints = []\n",
    "for review in word_reviews:\n",
    "    reviews_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_to_ints = []\n",
    "\n",
    "for review in word_unlabeled:\n",
    "    unlabeled_to_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length 0\n",
      "Max review length 931\n"
     ]
    }
   ],
   "source": [
    "reviews_lens = Counter([len(x) for x in reviews_to_ints])\n",
    "print('Zero-length {}'.format(reviews_lens[0]))\n",
    "print(\"Max review length {}\".format(max(reviews_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 250\n",
    "\n",
    "features = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(reviews_to_ints):\n",
    "    features[i, -len(review):] = np.array(review)[:seq_len]\n",
    "    \n",
    "features_test = np.zeros((len(unlabeled_to_ints), seq_len), dtype=int)\n",
    "for i, review in enumerate(unlabeled_to_ints):\n",
    "    features_test[i, -len(review):] = np.array(review)[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_trian shape (6400, 250)\n",
      "X_unlabeled shape (28936, 250)\n"
     ]
    }
   ],
   "source": [
    "X_train = features[:6400]\n",
    "y_train = labels[:6400]\n",
    "\n",
    "X_test = features[6400:]\n",
    "y_test = labels[6400:]\n",
    "\n",
    "X_unlabeled = features_test\n",
    "\n",
    "print('X_trian shape {}'.format(X_train.shape))\n",
    "print('X_unlabeled shape {}'.format(X_unlabeled.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ... 141  14 254]\n",
      " [  0   0   0 ...  40  41  45]\n",
      " [  0   0   0 ...   5 153 237]\n",
      " ...\n",
      " [  0   0   0 ...   1  49 223]\n",
      " [  0   0   0 ...  22   6 433]\n",
      " [  0   0   0 ...  44  14  31]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 512 # how many nodes LSTM cells will have\n",
    "number_of_layers = 1 # how many RNN layers the network will use\n",
    "batch_size = 100 # how many reviews we feed at onces\n",
    "learning_rate = 0.002 # learning rate\n",
    "number_of_words = len(vocab_to_int) + 1 #how many unique words do we have in vocab (+1  is used for 0 - padding)\n",
    "dropout_rate = 0.8 \n",
    "embed_size = 200 #how long our word embedings will be\n",
    "epochs = 4 # how many epochs do we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #Clean the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, [None, None], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedings = tf.Variable(tf.random_uniform((number_of_words, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(word_embedings, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size)\n",
    "hidden_layer = tf.contrib.rnn.DropoutWrapper(hidden_layer, dropout_rate)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([hidden_layer]*number_of_layers)\n",
    "init_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, embed, initial_state=init_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.layers.dense(outputs[:, -1], 1, activation=tf.sigmoid)\n",
    "cost = tf.losses.mean_squared_error(targets, prediction)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "currect_pred = tf.equal(tf.cast(tf.round(prediction), tf.int32), targets)\n",
    "accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/4  | Current loss: 0.048528365790843964  | Training accuracy: 93.2188\n",
      "Epoch: 1/4  | Current loss: 0.009912846609950066  | Training accuracy: 98.6719\n",
      "Epoch: 2/4  | Current loss: 0.003241236787289381  | Training accuracy: 99.6250\n",
      "Epoch: 3/4  | Current loss: 0.0035140272229909897  | Training accuracy: 99.5312\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    training_accurcy = []\n",
    "    ii = 0\n",
    "    epoch_loss = []\n",
    "    while ii + batch_size <= len(X_train):\n",
    "        X_batch = X_train[ii:ii+batch_size]\n",
    "        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n",
    "        \n",
    "        a, o, _ = session.run([accuracy, cost, optimizer], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "\n",
    "        training_accurcy.append(a)\n",
    "        epoch_loss.append(o)\n",
    "        ii += batch_size\n",
    "    print('Epoch: {}/{}'.format(i, epochs), ' | Current loss: {}'.format(np.mean(epoch_loss)),\n",
    "          ' | Training accuracy: {:.4f}'.format(np.mean(training_accurcy)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = []\n",
    "\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_test):\n",
    "    X_batch = X_test[ii:ii+batch_size]\n",
    "    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    a = session.run([accuracy], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    test_accuracy.append(a)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 98.6000%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy is {:.4f}%\".format(np.mean(test_accuracy)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unlabeled = []\n",
    "ii = 0\n",
    "while ii + batch_size <= len(X_unlabeled):\n",
    "    if ii + batch_size > len(X_unlabeled):\n",
    "        batch_size = len(X_unlabeled) - ii\n",
    "    X_batch = X_unlabeled[ii:ii+batch_size]\n",
    "    y_batch = X_unlabeled[ii:ii+batch_size].reshape(-1, 1)\n",
    "\n",
    "    pred = session.run([prediction], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "    \n",
    "    predictions_unlabeled.append(pred)\n",
    "    ii += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_real = []\n",
    "for i in range(len(predictions_unlabeled)):\n",
    "    for ii in range(len(predictions_unlabeled[i][0])):\n",
    "        if predictions_unlabeled[i][0][ii][0] >= 0.5:\n",
    "            pred_real.append(1)\n",
    "        else:\n",
    "            pred_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', pred_real)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = unlabeld_data[:len(pred_real)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shashwat.gupta\\AppData\\Local\\Continuum\\anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "new_dataframe['Classes'] = pred_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16955</th>\n",
       "      <td>I absolutely love my MacBook Pro.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17408</th>\n",
       "      <td>And as stupid as San Francisco's road system i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16562</th>\n",
       "      <td>TAKE THAT STUPID UCLA!!!!!!!..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8628</th>\n",
       "      <td>The piccs liked our dances = ) I ’ m a little ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27905</th>\n",
       "      <td>we need at least ONE beautiful girl at ucla.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>and honda elements are assholes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14831</th>\n",
       "      <td>I hate San Francisco for that.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17083</th>\n",
       "      <td>I loved the Lakers episode where he told that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>I love Tom Cruise!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25674</th>\n",
       "      <td>damn, seattle sucks.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>i love in seattle..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26878</th>\n",
       "      <td>And I hate Shanghai summers too...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>I want a tour of London on the back of that bike!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>ucla is so beautiful!!!!!!..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>the familiarity of San Francisco, and the amaz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19969</th>\n",
       "      <td>i love shanghai ~ ~ ~ ~ 外滩好像有一家专卖上海纪念品的小店 ~ ~ ~.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27823</th>\n",
       "      <td>Now I'm walking around looking like the uglies...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>I miss Hyundai Department store with all the f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>This past weekend was KrAzY with a CAPITAL K! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>When Emmy was in town, I told her ( in a momen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>For those who say he doesn't deserve it cuz th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21856</th>\n",
       "      <td>I like Tom Cruise and will boycott Paramount P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>iwent to harvard for a drama tournament last w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19626</th>\n",
       "      <td>Way to go stupid Lakers..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2335</th>\n",
       "      <td>i miss AAA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6270</th>\n",
       "      <td>And as stupid as San Francisco's road system i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14179</th>\n",
       "      <td>stupid lakers.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22262</th>\n",
       "      <td>Now I can totally rub it in my dad's face that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16154</th>\n",
       "      <td>but the macbook looks so awesome.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25237</th>\n",
       "      <td>No fucking joke I hate UCLA and that bullshit,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18472</th>\n",
       "      <td>United Airlines kind of sucks, what with their...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7452</th>\n",
       "      <td>I love MIT so much...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8069</th>\n",
       "      <td>Have I told you I hate Tom Cruise?..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8681</th>\n",
       "      <td>AAA's \" Q \" is catchy and an ear worm, like ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17379</th>\n",
       "      <td>I needed San Francisco but not until a month ago.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15989</th>\n",
       "      <td>Wow, having XP on my MacBook would be awesome...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21545</th>\n",
       "      <td>Cs: I've heard San Francisco is beautiful..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20297</th>\n",
       "      <td>i hate the lakers..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24793</th>\n",
       "      <td>I also love the new rabbits, I still want an x...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>But I miss Boston.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20732</th>\n",
       "      <td>Before I left Missouri, I thought London was g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>Paris Hilton – Sexy, relevant to a different k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27088</th>\n",
       "      <td>This means we beat out schools like MIT, which...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>I would love to go Shanghai with sister..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20973</th>\n",
       "      <td>I hate Purdue.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14430</th>\n",
       "      <td>we need at least ONE beautiful girl at ucla.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17178</th>\n",
       "      <td>I hated Boston.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>then again i hate tom cruise..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26790</th>\n",
       "      <td>Previously, I have installed Windoze just so I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13426</th>\n",
       "      <td>stupid ass toyota camry...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22003</th>\n",
       "      <td>= D i love Tom Cruise!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>I like this 96 Volkswagen GTI thats all modifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13285</th>\n",
       "      <td>i miss AAA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23450</th>\n",
       "      <td>. I despise Tom Cruise, love South Park, and h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26201</th>\n",
       "      <td>I love MIT so much...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11304</th>\n",
       "      <td>mostly these stupid guys with their shitty hon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>I've been working on an article, and Antid Oto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>I like being at Purdue, not with my family.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>BOSTON SUCKS!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19094</th>\n",
       "      <td>When I saw the letter that said 19,000 people ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data  Classes\n",
       "16955                  I absolutely love my MacBook Pro.        1\n",
       "17408  And as stupid as San Francisco's road system i...        0\n",
       "16562                     TAKE THAT STUPID UCLA!!!!!!!..        0\n",
       "8628   The piccs liked our dances = ) I ’ m a little ...        1\n",
       "27905       we need at least ONE beautiful girl at ucla.        1\n",
       "6941                  and honda elements are assholes...        0\n",
       "14831                     I hate San Francisco for that.        0\n",
       "17083  I loved the Lakers episode where he told that ...        1\n",
       "1045                                  I love Tom Cruise!        1\n",
       "25674                               damn, seattle sucks.        0\n",
       "307                                  i love in seattle..        1\n",
       "26878                 And I hate Shanghai summers too...        0\n",
       "4915   I want a tour of London on the back of that bike!        1\n",
       "10019                       ucla is so beautiful!!!!!!..        1\n",
       "4024   the familiarity of San Francisco, and the amaz...        1\n",
       "19969   i love shanghai ~ ~ ~ ~ 外滩好像有一家专卖上海纪念品的小店 ~ ~ ~.        1\n",
       "27823  Now I'm walking around looking like the uglies...        1\n",
       "11260  I miss Hyundai Department store with all the f...        0\n",
       "14188  This past weekend was KrAzY with a CAPITAL K! ...        1\n",
       "3529   When Emmy was in town, I told her ( in a momen...        1\n",
       "12472  For those who say he doesn't deserve it cuz th...        1\n",
       "21856  I like Tom Cruise and will boycott Paramount P...        1\n",
       "13853  iwent to harvard for a drama tournament last w...        1\n",
       "19626                          Way to go stupid Lakers..        0\n",
       "2335                                       i miss AAA...        1\n",
       "6270   And as stupid as San Francisco's road system i...        0\n",
       "14179                                 stupid lakers.....        0\n",
       "22262  Now I can totally rub it in my dad's face that...        1\n",
       "16154                  but the macbook looks so awesome.        1\n",
       "25237  No fucking joke I hate UCLA and that bullshit,...        0\n",
       "...                                                  ...      ...\n",
       "18472  United Airlines kind of sucks, what with their...        0\n",
       "7452                               I love MIT so much...        1\n",
       "8069                Have I told you I hate Tom Cruise?..        0\n",
       "8681   AAA's \" Q \" is catchy and an ear worm, like ma...        1\n",
       "17379  I needed San Francisco but not until a month ago.        1\n",
       "15989   Wow, having XP on my MacBook would be awesome...        1\n",
       "21545        Cs: I've heard San Francisco is beautiful..        1\n",
       "20297                                i hate the lakers..        0\n",
       "24793  I also love the new rabbits, I still want an x...        1\n",
       "6328                                  But I miss Boston.        1\n",
       "20732  Before I left Missouri, I thought London was g...        1\n",
       "2842   Paris Hilton – Sexy, relevant to a different k...        1\n",
       "27088  This means we beat out schools like MIT, which...        1\n",
       "2690           I would love to go Shanghai with sister..        1\n",
       "20973                                     I hate Purdue.        0\n",
       "14430       we need at least ONE beautiful girl at ucla.        1\n",
       "17178                                    I hated Boston.        0\n",
       "655                       then again i hate tom cruise..        0\n",
       "26790  Previously, I have installed Windoze just so I...        1\n",
       "13426                         stupid ass toyota camry...        0\n",
       "22003                           = D i love Tom Cruise!!!        1\n",
       "2209   I like this 96 Volkswagen GTI thats all modifi...        1\n",
       "13285                                      i miss AAA...        1\n",
       "23450  . I despise Tom Cruise, love South Park, and h...        1\n",
       "26201                              I love MIT so much...        1\n",
       "11304  mostly these stupid guys with their shitty hon...        0\n",
       "3847   I've been working on an article, and Antid Oto...        0\n",
       "8375         I like being at Purdue, not with my family.        1\n",
       "979                                      BOSTON SUCKS!!!        0\n",
       "19094  When I saw the letter that said 19,000 people ...        1\n",
       "\n",
       "[28900 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
